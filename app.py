import streamlit as st
import openai
import os
import json
import hashlib
import time
from uuid import uuid4

# Inicializa o cliente OpenAI com a chave da API
client = openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# Define os caminhos para o PDF, arquivo de configura√ß√£o do assistente e diret√≥rio de threads
PDF_PATH = "PDF/documento_ppa.pdf"
ASSISTANT_CONFIG_PATH = "assistant_config.json"
THREADS_DIR = "threads"
os.makedirs(THREADS_DIR, exist_ok=True)

# üîê Calcula o hash de um arquivo para verificar modifica√ß√µes
def hash_file(path):
    with open(path, "rb") as f:
        return hashlib.sha256(f.read()).hexdigest()

# üìå Cria ou carrega o Assistant e associa o arquivo PDF atrav√©s de um Vector Store
def get_or_create_assistant():
    current_hash = hash_file(PDF_PATH)

    # Tenta carregar a configura√ß√£o existente do assistente
    if os.path.exists(ASSISTANT_CONFIG_PATH):
        try:
            with open(ASSISTANT_CONFIG_PATH, "r") as f:
                config = json.load(f)
            # Verifica se o PDF foi modificado ou se a configura√ß√£o est√° incompleta
            if (config.get("pdf_hash") == current_hash and
                config.get("vector_store_id") and
                config.get("assistant_id") and
                config.get("file_id")): # Garante que file_id tamb√©m existe
                
                # Tenta recuperar objetos para garantir que ainda s√£o v√°lidos na OpenAI
                try:
                    client.beta.assistants.retrieve(config["assistant_id"])
                    # Usa client.vector_stores para recupera√ß√£o tamb√©m
                    client.vector_stores.retrieve(config["vector_store_id"])
                    client.files.retrieve(config["file_id"])
                    return config["assistant_id"], config["file_id"], config["vector_store_id"]
                except openai.NotFoundError:
                    st.warning("Assistente, Vector Store ou Arquivo n√£o encontrados na OpenAI. Recriando...")
                    os.remove(ASSISTANT_CONFIG_PATH) # For√ßa a recria√ß√£o se os objetos desapareceram

            else:
                # Se o PDF mudou ou a config est√° incompleta/corrompida, remove o arquivo de config para recriar
                st.warning("Configura√ß√£o do assistente desatualizada ou incompleta. Recriando...")
                os.remove(ASSISTANT_CONFIG_PATH)
        except json.JSONDecodeError:
            st.warning("Arquivo de configura√ß√£o do assistente corrompido. Recriando...")
            os.remove(ASSISTANT_CONFIG_PATH)
        except Exception as e:
            st.error(f"Erro inesperado ao carregar configura√ß√£o do assistente: {e}. Recriando...")
            os.remove(ASSISTANT_CONFIG_PATH)


    # Se n√£o existe config v√°lida, cria tudo do zero

    st.info("Configurando o assistente pela primeira vez ou atualizando PDF. Isso pode levar alguns segundos...")

    # 1. Upload do arquivo PDF para a OpenAI
    with open(PDF_PATH, "rb") as f:
        uploaded_file = client.files.create(file=f, purpose="assistants")

    # 2. Cria um Vector Store (AGORA DIRETAMENTE SOB CLIENT, N√ÉO client.beta)
    vector_store = client.vector_stores.create(name="VectorStore-PPA")

    # 3. Adiciona o arquivo carregado ao Vector Store
    # Aguarda a conclus√£o do processamento do arquivo no Vector Store
    file_batch = client.vector_stores.file_batches.create(
        vector_store_id=vector_store.id,
        file_ids=[uploaded_file.id]
    )

    # Espera at√© que o batch de arquivos seja processado
    with st.spinner("Processando o documento no Vector Store..."):
        while True:
            # Usa client.vector_stores para recuperar o status do batch
            file_batch = client.vector_stores.file_batches.retrieve(
                vector_store_id=vector_store.id,
                batch_id=file_batch.id,
            )
            if file_batch.status in ["completed", "failed", "cancelled"]:
                break
            time.sleep(1) # Espera 1 segundo antes de tentar novamente

    if file_batch.status != "completed":
        st.error(f"Falha ao processar o arquivo no Vector Store. Status: {file_batch.status}. Por favor, tente novamente.")
        st.stop()

    # 4. Cria o Assistant, associando-o ao Vector Store (Assistants ainda est√° em beta)
    assistant = client.beta.assistants.create(
        name="Assistente do PPA",
        instructions=(
            "Voc√™ √© um assistente de IA amig√°vel e acess√≠vel, especializado no Programa Plurianual (PPA) do governo do Brasil. "
            "Sua miss√£o √© ajudar o cidad√£o a entender o PPA de forma simples, respondendo a perguntas **exclusivamente com base no documento PPA fornecido**. "
            "**Sua maior prioridade √© diferenciar e apresentar claramente os 'Objetivos Estrat√©gicos' e os 'Objetivos Espec√≠ficos' quando solicitado.** "
            " - **Objetivos Estrat√©gicos** s√£o a vis√£o de alto n√≠vel, os grandes prop√≥sitos ou dire√ß√µes. "
            " - **Objetivos Espec√≠ficos** s√£o os passos mais detalhados, concretos e mensur√°veis para alcan√ßar os objetivos estrat√©gicos. "
            "Quando perguntado sobre 'objetivos estrat√©gicos', forne√ßa os pontos de alto n√≠vel. "
            "Quando perguntado sobre 'objetivos espec√≠ficos', detalhe os resultados mais pr√°ticos e direcionados. "
            "Se o cidad√£o perguntar sobre 'objetivos' de forma geral, tente identificar se ele busca a vis√£o ampla (estrat√©gica) ou os detalhes (espec√≠ficos) e responda de forma apropriada, talvez oferecendo ambos se a pergunta for amb√≠gua. "
            "**Se a informa√ß√£o ou a distin√ß√£o exata n√£o estiver no documento, ou se voc√™ n√£o conseguir diferenciar com clareza com base nele, diga explicitamente que a informa√ß√£o n√£o foi encontrada ou n√£o est√° clara no documento fornecido, evitando criar respostas ou 'alucinar'.** "
            "Seja sempre did√°tico, claro e evite jarg√µes t√©cnicos. Mantenha as respostas concisas, mas completas para a pergunta."
        ),
        model="gpt-4-1106-preview",
        tools=[{"type": "file_search"}],
        tool_resources={
            "file_search": {
                "vector_store_ids": [vector_store.id]
            }
        }
    )

    # Salva os IDs do assistente, arquivo e vector store, junto com o hash do PDF
    with open(ASSISTANT_CONFIG_PATH, "w") as f:
        json.dump({
            "assistant_id": assistant.id,
            "file_id": uploaded_file.id,
            "vector_store_id": vector_store.id,
            "pdf_hash": current_hash
        }, f)

    return assistant.id, uploaded_file.id, vector_store.id


# üßµ Cria ou carrega uma thread de conversa√ß√£o para o usu√°rio
def get_or_create_thread():
    if "user_id" not in st.session_state:
        st.session_state.user_id = str(uuid4()) # Gera um ID de usu√°rio √∫nico se n√£o existir
    thread_path = os.path.join(THREADS_DIR, f"{st.session_state.user_id}.json")
    if os.path.exists(thread_path):
        try:
            with open(thread_path, "r") as f:
                thread_id = json.load(f)["thread_id"]
                client.beta.threads.retrieve(thread_id) # Valida se a thread ainda existe na OpenAI
                return thread_id
        except (json.JSONDecodeError, openai.NotFoundError):
            st.warning("Thread de conversa corrompida ou n√£o encontrada. Criando nova thread...")
            os.remove(thread_path) # For√ßa a cria√ß√£o de uma nova thread
    
    thread = client.beta.threads.create() # Cria uma nova thread
    with open(thread_path, "w") as f:
        json.dump({"thread_id": thread.id}, f)
    return thread.id

# üí¨ Exibe o hist√≥rico de mensagens da thread
def show_history(thread_id):
    messages = client.beta.threads.messages.list(thread_id=thread_id, order="asc") # Obt√©m mensagens em ordem ascendente
    for msg in messages.data:
        # Garante que o conte√∫do √© do tipo text e n√£o outros tipos (ex: image_file)
        if msg.content and hasattr(msg.content[0], 'text') and msg.content[0].text:
            role = "üë§ Cidad√£o" if msg.role == "user" else "ü§ñ Assistente"
            content = msg.content[0].text.value
            st.chat_message(role).markdown(content)

# üöÄ In√≠cio do aplicativo Streamlit
st.set_page_config(page_title="Assistente do PPA", page_icon="üìÑ", layout="wide")
st.title("üìÑ Pergunte sobre o Programa Plurianual (PPA) do Governo")

# Dicas para o usu√°rio final sobre como perguntar
st.markdown("### Dicas para fazer sua pergunta:")
st.markdown("- Tente perguntar sobre um tema espec√≠fico, como 'Me fale sobre a sa√∫de no PPA'.")
st.markdown("- Se quiser saber sobre os planos maiores, pergunte 'Quais s√£o os **objetivos gerais** do PPA?'")
st.markdown("- Se quiser detalhes, pergunte 'Quais s√£o os **objetivos espec√≠ficos** para educa√ß√£o?'")
st.markdown("---") # Linha divis√≥ria para separar as dicas do chat

# Obt√©m ou cria o assistente e a thread de conversa√ß√£o
assistant_id, file_id, vector_store_id = get_or_create_assistant()
thread_id = get_or_create_thread()

# Exibe o hist√≥rico da conversa ao iniciar
show_history(thread_id)

# Campo de entrada para a pergunta do usu√°rio
if user_input := st.chat_input("Digite sua pergunta sobre o PPA..."):
    st.chat_message("üë§ Cidad√£o").markdown(user_input)

    # Adiciona a mensagem do usu√°rio √† thread (sem file_ids aqui, pois o assistente j√° tem acesso ao arquivo via vector store)
    client.beta.threads.messages.create(
        thread_id=thread_id,
        role="user",
        content=user_input,
    )

    # Cria uma "run" para que o assistente processe a mensagem e gere uma resposta
    run = client.beta.threads.runs.create(
        thread_id=thread_id,
        assistant_id=assistant_id
    )

    # Loop para aguardar a conclus√£o da "run"
    with st.spinner("Consultando o PPA..."):
        while True:
            run_status = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run.id)
            if run_status.status == "completed":
                break
            elif run_status.status == "failed":
                # Mensagem de erro mais amig√°vel para o cidad√£o
                st.error("Desculpe, n√£o consegui encontrar uma resposta no documento do PPA para sua pergunta neste momento. Por favor, tente reformular sua pergunta ou perguntar sobre outro t√≥pico.")
                if run_status.last_error: # Opcional: Para depura√ß√£o, voc√™ pode querer ver o erro real no console/logs
                    print(f"Erro detalhado da OpenAI: {run_status.last_error.message}")
                st.stop()
            elif run_status.status == "requires_action":
                st.warning("O assistente requer uma a√ß√£o. Fun√ß√µes de ferramenta podem ser necess√°rias (funcionalidade avan√ßada n√£o implementada nesta vers√£o).")
                break
            time.sleep(1) # Espera um pouco antes de verificar novamente

        # Recupera as mensagens mais recentes e exibe a resposta do assistente
        messages = client.beta.threads.messages.list(thread_id=thread_id, order="desc")
        # Itera sobre as mensagens para encontrar a √∫ltima do assistente
        for msg in messages.data:
            if msg.role == "assistant" and msg.content and hasattr(msg.content[0], 'text') and msg.content[0].text:
                st.chat_message("ü§ñ Assistente").markdown(msg.content[0].text.value)
                break # Sai do loop ap√≥s encontrar e exibir a primeira mensagem do assistente
